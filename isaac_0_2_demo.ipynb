{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Isaac-0.2 FiftyOne Integration Demo\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/perceptron-ai-inc/fiftyone-isaac-0.2/blob/main/isaac_0_2_demo.ipynb)\n\nThis notebook demonstrates how to use Isaac-0.2 by Perceptron AI with FiftyOne for various computer vision tasks including object detection, OCR, classification, and visual question answering.\n\n## About Isaac-0.2\n\nIsaac-0.2 is an open-source, 2B-parameter perceptive-language model designed for real-world visual understanding tasks. It delivers capabilities comparable to models 50x larger while being efficient enough for practical applications."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, let's install the required dependencies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q fiftyone\n",
    "%pip install -q perceptron\n",
    "%pip install -q transformers\n",
    "%pip install -q torch torchvision\n",
    "%pip install -q huggingface-hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's import the necessary libraries and suppress warnings for cleaner output:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from transformers import logging\n",
    "\n",
    "# Suppress transformers warnings for cleaner output\n",
    "logging.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.utils.huggingface as fouh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Register and Load Isaac-0.2 Model\n\nFirst, we need to register the Isaac-0.2 model zoo source and then load the model:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Register the Isaac-0.2 model zoo source\nfoz.register_zoo_model_source(\n    \"https://github.com/perceptron-ai-inc/fiftyone-isaac-0.2\",\n    overwrite=True\n)\n\n# Load the Isaac-0.2 model\nprint(\"Loading Isaac-0.2 model...\")\nmodel = foz.load_zoo_model(\"PerceptronAI/Isaac-0.2-2B-Preview\")\nprint(\"Model loaded successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Testing on Generic Images\n",
    "\n",
    "Let's load a sample dataset and test various operations on generic images:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample dataset from Hugging Face\n",
    "dataset = fouh.load_from_hub(\n",
    "    \"Voxel51/GQA-Scene-Graph\",\n",
    "    max_samples=50,  # Using fewer samples for demo\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(dataset)} samples\")\n",
    "print(f\"First sample fields: {dataset.first().field_names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique object labels from the dataset\n",
    "sample_objects = dataset.values(\"detections.detections.label\")\n",
    "sample_level_objects = [list(set(obj)) if obj else [] for obj in sample_objects]\n",
    "dataset.set_values(\"sample_level_objects\", sample_level_objects)\n",
    "\n",
    "# Display a sample of objects found\n",
    "print(\"Sample objects found in first image:\", sample_level_objects[0][:10] if sample_level_objects[0] else \"None\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.1 Visual Question Answering (VQA)\n\nLet's use Isaac-0.2 to answer questions about the images:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to VQA mode\n",
    "model.operation = \"vqa\"\n",
    "print(f\"System prompt for VQA:\\n{model.system_prompt}\\n\")\n",
    "\n",
    "# Set the question prompt\n",
    "model.prompt = \"Provide a short description of the spatial relationships between the objects in this scene\"\n",
    "\n",
    "# Apply the model to a subset of samples\n",
    "print(\"Running VQA on dataset...\")\n",
    "dataset.apply_model(model, label_field=\"vqa_description\")\n",
    "\n",
    "# Display results\n",
    "first_sample = dataset.first()\n",
    "print(f\"VQA Result for first image:\\n{first_sample.vqa_description}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Object Detection\n",
    "\n",
    "Now let's detect objects in the images:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to detection mode\n",
    "model.operation = \"detect\"\n",
    "print(f\"System prompt for detection:\\n{model.system_prompt[:200]}...\\n\")\n",
    "\n",
    "# Use sample-level prompts for detection (objects from the ground truth)\n",
    "print(\"Running object detection using sample-level prompts...\")\n",
    "dataset.apply_model(\n",
    "    model, \n",
    "    label_field=\"isaac_detections\", \n",
    "    prompt_field=\"sample_level_objects\"\n",
    ")\n",
    "\n",
    "# Display detection results\n",
    "first_sample = dataset.first()\n",
    "if first_sample.isaac_detections and first_sample.isaac_detections.detections:\n",
    "    print(f\"Detected {len(first_sample.isaac_detections.detections)} objects in first image:\")\n",
    "    for det in first_sample.isaac_detections.detections[:5]:  # Show first 5\n",
    "        print(f\"  - {det.label}\")\n",
    "else:\n",
    "    print(\"No detections in first image\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Keypoint Detection\n",
    "\n",
    "Let's identify key points in the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to keypoint detection mode\n",
    "model.operation = \"point\"\n",
    "print(f\"System prompt for keypoints:\\n{model.system_prompt[:200]}...\\n\")\n",
    "\n",
    "# Apply keypoint detection using sample-level prompts\n",
    "print(\"Running keypoint detection...\")\n",
    "dataset.limit(10).apply_model(\n",
    "    model, \n",
    "    label_field=\"isaac_keypoints\", \n",
    "    prompt_field=\"sample_level_objects\"\n",
    ")\n",
    "\n",
    "# Display keypoint results\n",
    "first_sample = dataset.first()\n",
    "if first_sample.isaac_keypoints and first_sample.isaac_keypoints.keypoints:\n",
    "    print(f\"Detected {len(first_sample.isaac_keypoints.keypoints)} keypoints in first image:\")\n",
    "    for kp in first_sample.isaac_keypoints.keypoints[:5]:  # Show first 5\n",
    "        print(f\"  - {kp.label}\")\n",
    "else:\n",
    "    print(\"No keypoints detected in first image\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Image Classification\n",
    "\n",
    "Let's classify the weather/environment in the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to classification mode\n",
    "model.operation = \"classify\"\n",
    "print(f\"System prompt for classification:\\n{model.system_prompt[:200]}...\\n\")\n",
    "\n",
    "# Set classification prompt\n",
    "model.prompt = \"Classify the weather/environment in this scene into exactly one of the following: sunny, rainy, snowy, cloudy, indoor\"\n",
    "\n",
    "# Apply classification\n",
    "print(\"Running classification...\")\n",
    "dataset.limit(10).apply_model(model, label_field=\"weather_classification\")\n",
    "\n",
    "# Display classification results\n",
    "first_sample = dataset.first()\n",
    "if first_sample.weather_classification and first_sample.weather_classification.classifications:\n",
    "    print(f\"Weather classification for first image:\")\n",
    "    for cls in first_sample.weather_classification.classifications:\n",
    "        print(f\"  - {cls.label}\")\n",
    "else:\n",
    "    print(\"No classification for first image\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Drawing polygons\n",
    "\n",
    "Let's try to generate some segmentation masks by prompting the model to produce polygons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.operation=\"segment\"\n",
    "\n",
    "print(model.system_prompt)\n",
    "\n",
    "dataset.apply_model(model, label_field=\"pf_segments\", prompt_field=\"sample_level_objects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 2: Testing on Text Images (OCR)\n\nNow let's test Isaac-0.2's OCR capabilities on images containing text:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset with text images\n",
    "print(\"Loading text dataset...\")\n",
    "text_dataset = fouh.load_from_hub(\n",
    "    \"Voxel51/Total-Text-Dataset\",\n",
    "    max_samples=20  # Using fewer samples for demo\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(text_dataset)} text samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 OCR Text Extraction\n",
    "\n",
    "Extract text content from images:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to OCR mode for text extraction\n",
    "model.operation = \"ocr\"\n",
    "print(f\"System prompt for OCR:\\n{model.system_prompt[:200]}...\\n\")\n",
    "\n",
    "# Set OCR prompt\n",
    "model.prompt = \"Report all text visible in this image\"\n",
    "\n",
    "# Apply OCR to extract text\n",
    "print(\"Running OCR text extraction...\")\n",
    "text_dataset.limit(10).apply_model(model, label_field=\"extracted_text\")\n",
    "\n",
    "# Display results\n",
    "first_text_sample = text_dataset.first()\n",
    "if first_text_sample.extracted_text:\n",
    "    print(f\"Extracted text from first image:\\n{first_text_sample.extracted_text[:200]}...\")\n",
    "else:\n",
    "    print(\"No text extracted from first image\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 OCR Text Detection\n",
    "\n",
    "Detect text regions with bounding boxes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to OCR detection mode\n",
    "model.operation = \"ocr_detection\"\n",
    "print(f\"System prompt for OCR detection:\\n{model.system_prompt[:200]}...\\n\")\n",
    "\n",
    "# Set OCR detection prompt\n",
    "model.prompt = \"Detect all text regions in this image\"\n",
    "\n",
    "# Apply OCR detection\n",
    "print(\"Running OCR text detection...\")\n",
    "text_dataset.limit(10).apply_model(model, label_field=\"text_regions\")\n",
    "\n",
    "# Display detection results\n",
    "first_text_sample = text_dataset.first()\n",
    "if first_text_sample.text_regions and first_text_sample.text_regions.detections:\n",
    "    print(f\"Detected {len(first_text_sample.text_regions.detections)} text regions in first image:\")\n",
    "    for det in first_text_sample.text_regions.detections[:5]:  # Show first 5\n",
    "        print(f\"  - '{det.label}'\")\n",
    "else:\n",
    "    print(\"No text regions detected in first image\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Part 3: Advanced Model Options\n\nIsaac-0.2 supports advanced options to customize model behavior:\n\n- **`enable_thinking`**: Enables chain-of-thought reasoning. The model will reason through the task before producing output, which can improve accuracy for complex scenes. Works with all operations.\n\n- **`enable_focus_tool_call`**: Enables focused tool calling for more precise detection. Only works with BOX hint operations (`detect`, `ocr_detection`).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Enable thinking mode for object detection\nmodel.operation = \"detect\"\nmodel.enable_thinking = True\n\nprint(\"Thinking mode enabled - model will reason before detecting objects\")\n\n# Run detection with thinking on a few samples\ndataset.limit(5).apply_model(\n    model, \n    label_field=\"isaac_detections_with_thinking\", \n    prompt_field=\"sample_level_objects\"\n)\n\n# Display results\nfirst_sample = dataset.first()\nif first_sample.isaac_detections_with_thinking and first_sample.isaac_detections_with_thinking.detections:\n    print(f\"Detected {len(first_sample.isaac_detections_with_thinking.detections)} objects with thinking:\")\n    for det in first_sample.isaac_detections_with_thinking.detections[:5]:\n        print(f\"  - {det.label}\")\n\n# Disable thinking mode for subsequent operations\nmodel.enable_thinking = False",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Enable focus tool call for more precise detection\n# Note: enable_focus_tool_call only works with BOX hint operations (detect, ocr_detection)\nmodel.operation = \"detect\"\nmodel.enable_focus_tool_call = True\n\nprint(\"Focus tool call enabled - model will use focused detection tools\")\n\n# Run detection with focus tool call on a few samples\ndataset.limit(5).apply_model(\n    model, \n    label_field=\"isaac_detections_focused\", \n    prompt_field=\"sample_level_objects\"\n)\n\n# Display results\nfirst_sample = dataset.first()\nif first_sample.isaac_detections_focused and first_sample.isaac_detections_focused.detections:\n    print(f\"Detected {len(first_sample.isaac_detections_focused.detections)} objects with focus tool call:\")\n    for det in first_sample.isaac_detections_focused.detections[:5]:\n        print(f\"  - {det.label}\")\n\n# Disable focus tool call for subsequent operations\nmodel.enable_focus_tool_call = False",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Explore in the FiftyOne App\n\nYou can launch the FiftyOne app via command like: `fiftyone app launch`\n\nOr start a session in your notebooks:\n\n```python\nimport fiftyone as fo\n\nfo.launch_app(dataset)\n```\n\nTo view the captions in a panel you can install the caption viewer plugin by running the following in your terminal:\n\n`fiftyone plugins download https://github.com/mythrandire/caption-viewer`\n\nCheckout our other plugins [here](https://github.com/voxel51/fiftyone-plugins?tab=readme-ov-file)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nIn this notebook, we demonstrated how to use Isaac-0.2 with FiftyOne for various computer vision tasks:\n\n1. **Visual Question Answering (VQA)** - Generated descriptions of spatial relationships\n2. **Object Detection** - Detected objects with bounding boxes\n3. **Keypoint Detection** - Identified key points in images\n4. **Classification** - Classified weather/environment conditions\n5. **OCR Text Extraction** - Extracted text content from images\n6. **OCR Text Detection** - Detected text regions with bounding boxes\n7. **Advanced Options** - Thinking mode and focus tool call for improved accuracy\n\nIsaac-0.2 is a powerful 2B-parameter model that delivers impressive results across all these tasks while being efficient enough for practical applications.\n\n## Resources\n\n- [Isaac-0.2 on Hugging Face](https://huggingface.co/PerceptronAI/Isaac-0.2-2B-Preview)\n- [Isaac-0.2 FiftyOne Integration](https://github.com/perceptron-ai-inc/fiftyone-isaac-0.2)\n- [Perceptron AI GitHub](https://github.com/perceptron-ai-inc/perceptron)\n- [FiftyOne Documentation](https://docs.voxel51.com/)\n\n## License\n\n- **Code**: Apache 2.0 License\n- **Model Weights**: Creative Commons Attribution-NonCommercial 4.0 International License"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}